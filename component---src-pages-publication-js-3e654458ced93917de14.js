(self.webpackChunkgatsby_starter_default=self.webpackChunkgatsby_starter_default||[]).push([[5113],{6179:function(e,t,l){"use strict";var a=l(7294),n=l(5414),r=l(5444);function i(e){var t,l,i=e.description,o=e.lang,c=e.meta,m=e.title,s=(0,r.K2)("63159454").site,u=i||s.siteMetadata.description,p=null===(t=s.siteMetadata)||void 0===t?void 0:t.title;return a.createElement(n.q,{htmlAttributes:{lang:o},title:m,titleTemplate:p?"%s | "+p:null,meta:[{name:"description",content:u},{property:"og:title",content:m},{property:"og:description",content:u},{property:"og:type",content:"website"},{name:"twitter:card",content:"summary"},{name:"twitter:creator",content:(null===(l=s.siteMetadata)||void 0===l?void 0:l.author)||""},{name:"twitter:title",content:m},{name:"twitter:description",content:u}].concat(c)})}i.defaultProps={lang:"en",meta:[],description:""},t.Z=i},4911:function(e,t,l){"use strict";l.r(t),l.d(t,{default:function(){return c}});var a=l(7294),n=(l(5444),l(7849)),r=l(6179),i="publication-module--pub_links--wfJXo",o=l(8519),c=function(){return a.createElement(n.Z,null,a.createElement(r.Z,{title:"Publications"}),a.createElement("h1",{onLoad:void 0},"Publications"),a.createElement("div",null,a.createElement("div",null,a.createElement("papertitle",null,"M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval"),a.createElement("br",null),"Layne Berry, ",a.createElement("strong",null,"Yi-Jen Shih"),", Hsuan-Fu Wang, Heng-Jui Chang, Hung-yi Lee, David Harwath",a.createElement("br",null),a.createElement("em",null,"Under Review ICASSP"),a.createElement("br",null),a.createElement("div",{className:i},a.createElement(o.Z,{size:"small",label:"arXiv",component:"a",href:"https://arxiv.org/abs/2211.01180",variant:"outlined",color:"primary",clickable:!0})),a.createElement("p",null)),a.createElement("div",null,a.createElement("papertitle",null,"SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model"),a.createElement("br",null),a.createElement("strong",null,"Yi-Jen Shih"),", Hsuan-Fu Wang, Heng-Jui Chang, Layne Berry, Hung-yi Lee, David Harwath",a.createElement("br",null),a.createElement("em",null,"IEEE Spoken Language Technology Workshop (SLT) 2022"),a.createElement("br",null),a.createElement("div",{className:i},a.createElement(o.Z,{size:"small",label:"arXiv",component:"a",href:"https://arxiv.org/abs/2210.00705",variant:"outlined",color:"primary",clickable:!0}),a.createElement(o.Z,{size:"small",label:"code",component:"a",href:"https://github.com/atosystem/SpeechCLIP",variant:"outlined",color:"primary",clickable:!0}),a.createElement(o.Z,{size:"small",label:"blog",component:"a",href:"blogs/speechclip",variant:"outlined",color:"primary",clickable:!0})),a.createElement("p",null)),a.createElement("div",null,a.createElement("papertitle",null,"Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer"),a.createElement("br",null),a.createElement("strong",null,"Yi-Jen Shih"),", Shih-Lun Wu, Frank Zalkow, Meinard MÃ¼ller, Yi-Hsuan Yang",a.createElement("br",null),a.createElement("em",null,"IEEE Transactions on Multimedia (TMM) 2022"),a.createElement("br",null),a.createElement("div",{className:i},a.createElement(o.Z,{size:"small",label:"arXiv",component:"a",href:"https://arxiv.org/abs/2111.04093",variant:"outlined",color:"primary",clickable:!0}),a.createElement(o.Z,{size:"small",label:"code",component:"a",href:"https://github.com/atosystem/ThemeTransformer",variant:"outlined",color:"primary",clickable:!0}),a.createElement(o.Z,{size:"small",label:"demo",component:"a",href:"https://atosystem.github.io/ThemeTransformer/#demor",variant:"outlined",color:"primary",clickable:!0}),a.createElement(o.Z,{size:"small",label:"blog",component:"a",href:"blogs/theme-transformer",variant:"outlined",color:"primary",clickable:!0})),a.createElement("p",null))))}}}]);
//# sourceMappingURL=component---src-pages-publication-js-3e654458ced93917de14.js.map
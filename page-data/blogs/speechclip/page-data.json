{"componentChunkName":"component---src-templates-blog-details-js","path":"/blogs/speechclip","result":{"data":{"markdownRemark":{"html":"<h1>SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model</h1>\n<!-- <img height=\"600px\" src=\"./model_overall.png\"></img> -->\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 83.43558282208589%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAARABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAQDAgX/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAfa6m3l1SkrxCIZ1/8QAHRAAAQMFAQAAAAAAAAAAAAAAAQACAxAREjEzQf/aAAgBAQABBQL2mYRNmMkBCl5s1//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPwFqo//EABoQAAICAwAAAAAAAAAAAAAAAAABEDERIZL/2gAIAQEABj8CoqHt8syXDj//xAAeEAACAQMFAAAAAAAAAAAAAAAAASERMUEQYXGBof/aAAgBAQABPyGK4ie5CKrwnIKe+Fcuk+tPCX+T/9oADAMBAAIAAwAAABA7B73/xAAYEQACAwAAAAAAAAAAAAAAAAAAARAhUf/aAAgBAwEBPxBrC5//xAAXEQEBAQEAAAAAAAAAAAAAAAAAMQER/9oACAECAQE/EI4RqH//xAAeEAEAAgICAwEAAAAAAAAAAAABABEhMWHwEEFRkf/aAAgBAQABPxBGChW6ysyAAx7KIrBizNENDdfMx2kLEGfyWgytr8dbmdngn//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"model summary\"\n        title=\"model summary\"\n        src=\"/static/f11889a0456f3cc771e264a70fb49fe0/6aca1/model_summary.jpg\"\n        srcset=\"/static/f11889a0456f3cc771e264a70fb49fe0/d2f63/model_summary.jpg 163w,\n/static/f11889a0456f3cc771e264a70fb49fe0/c989d/model_summary.jpg 325w,\n/static/f11889a0456f3cc771e264a70fb49fe0/6aca1/model_summary.jpg 650w,\n/static/f11889a0456f3cc771e264a70fb49fe0/7c09c/model_summary.jpg 975w,\n/static/f11889a0456f3cc771e264a70fb49fe0/01ab0/model_summary.jpg 1300w,\n/static/f11889a0456f3cc771e264a70fb49fe0/55739/model_summary.jpg 7083w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h1>TL;DR</h1>\n<p>Parallel speech-text data is expensive and hard to collect, compared to paired speech-image, text-image data. To this end, we utilize the large scale pretrained image-language model, CLIP and speech self-supervised model, HuBERT to bridge speech and text together. Under several design of model architecture, we achieve SOTA on image-speech retrieval and we also show that SpeechCLIP can conduct zero-shot speech-text retrieval and keywords discovery from speech utternace.</p>\n<h1>Abstract</h1>\n<p>Data-driven speech processing models usually perform well with a large amount of text supervision, but collecting transcribed speech data is costly. Therefore, we propose SpeechCLIP, a novel framework bridging speech and text through images to enhance speech models without transcriptions. We leverage state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior state-of-the-art on image-speech retrieval and performs zero-shot speech-text retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP can directly retrieve semantically related keywords from speech.</p>\n<h1>Cite our work!</h1>","frontmatter":{"stack":"HTML & python","title":"SpeechCLIP","featuredImg":null}}},"pageContext":{"slug":"speechclip"}},"staticQueryHashes":["2366270877","3151475382","63159454"]}